
# TASK 1.C: QUESTION ANSWERING (NEWSQA DATASET)

This task experiments with building and fine-tuning a Question Answering (QA) model using the NewsQA dataset.

A transformer-based architecture  is fine-tuned to extract precise answers from news article passages.

## üß† Transformer for Question Answering

The model fine-tuned in this task is based on Hugging Face Transformer, a pre-trained RoBERTa-base (or BERT-base) architecture. RoBERTa stands for Robustly Optimized BERT Approach.


It is fine-tuned on the NewsQA dataset ‚Äî a large collection of news articles and associated comprehension-style question.

## üöÄ PROJECT OVERVIEW

The notebook builds a complete extractive QA pipeline using the Hugging Face ecosystem.

It performs:

1. Dataset loading from Hugging Face Datasets

2. Data preprocessing and context-question alignment

3. Tokenization and feature creation with overflow handling

4. Fine-tuning of a transformer model using Trainer

6. Inference for custom question-answering

## üì¶ DEPENDENCIES

The following libraries were used:

‚Ä¢ transformers ‚Äî for model, tokenizer, and training utilities

‚Ä¢ datasets ‚Äî to load and manage the NewsQA dataset

‚Ä¢ torch ‚Äî PyTorch backend for training

‚Ä¢ numpy & pandas ‚Äî for data manipulation

‚Ä¢ tqdm ‚Äî for progress visualization

Install all dependencies:

````markdown
pip install transformers datasets torch evaluate pandas numpy tqdm
````
 ## ü™ú PIPELINE STEPS

### 1. LOADING THE DATASET

Source: lucadiliello/newsqa from Hugging Face Datasets

Domain: News articles and comprehension-style questions

#### Structure:

1. context: context for the given question

2. question: question

3. answer: answer for the question

4. key: unique value to identify example

5. label: dict containing start and end of answer

````markdown
from datasets import load_dataset
ds = load_dataset("lucadiliello/newsqa")
````

### 2. üßπ DATA PREPARATION & PREPROCESSING

This is the most crucial stage of the QA pipeline. It ensures that the question‚Äìcontext pairs are correctly aligned and that each tokenized sample is properly labeled for training the model to extract the correct answer span.

#### üîß Key Steps:

1. Question Cleaning ‚Äî Removes leading/trailing spaces from the question text.

2. Tokenization with Sliding Window ‚Äî

‚Ä¢ Uses the tokenizer to split text into tokens with overlap (stride=128).

‚Ä¢ Ensures that long contexts exceeding max_length=384 are chunked properly.

3. Overflow Mapping ‚Äî Keeps track of which tokenized chunks belong to which original sample (overflow_to_sample_mapping).

4. Offset Mapping ‚Äî For each sub-token returned by the tokenizer, the offset mapping gives us a tuple indicating the sub-token‚Äôs start position and end position relative to the original token it was split from.

5. Answer Alignment ‚Äî

‚Ä¢ For each tokenized chunk, locates where the answer text lies.

‚Ä¢ Converts those into token indices (start_positions and end_positions) for model training.

6. Output ‚Äî Returns a dictionary containing:

‚Ä¢ Tokenized input (input_ids, attention_mask, etc.)

‚Ä¢ Corresponding start_positions and end_positions labels.

### 3. MODEL FINE-TUNING

#### Base Model:

‚Ä¢ Architecture: RoBERTa / BERT (encoder-only transformer)

‚Ä¢ Task: Extractive Question Answering

‚Ä¢ Pre-trained model: roberta-base

‚Ä¢ Parameters: ~125M

#### Training Parameters 

````markdown
from transformers import TrainingArguments

args = TrainingArguments(
    "roberta-base-finetuned",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=1,
    weight_decay=0.01,
    fp16=True,
    gradient_accumulation_steps=2,
)
````

### 5. MAKING PREDICTIONS

````markdown
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)

context = "amey have music club audition tommorow and he's gona get fucked real bad"
question = ["what does amey have tommorow?"]

result = qa_pipeline(
    question=question,
    context=context
)
print(result["answer"])
#output - music club audition
````

## üåü FUTURE IMPROVEMENTS

‚Ä¢ Use larger architectures (RoBERTa-large, DeBERTa-v3) for improved comprehension

‚Ä¢ Integrate early stopping and learning rate scheduling

‚Ä¢ Apply mixed-precision training (fp16) for faster computation

## üßë‚Äçüíª AUTHOR

Amey Bhagat

üìß amey.241ds009@gmail.com


